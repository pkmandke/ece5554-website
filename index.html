<!doctype html>
<html>

<head>
  <title>Deep Low Light Image Enhancement</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container">
    <div class="menu">
      <div class="menu-table flex-row-space-between">
        <div class="logo flex-row-center">
          <a href="index.html">Fall 2020 ECE 5554 Computer Vision</a>
        </div>
        <a class="menu-button" tabindex="0" href="javascript:void(0)">
          <img src="img/menu.png">
        </a>
        <div class="menu-items flex-row-center flex-item">
          <a href="#abstract" class="menu-index">Abstract</a>
          <a href="#introduction" class="menu-widget">Introduction</a>
          <a href="#approach" class="menu-widget">Approach</a>
          <a href="#exp-and-results" class="menu-embedding">Experiments</a>
          <a href="#conclusion" class="menu-widget">Conclusion</a>
          <a href="#references" class="menu-embedding">References</a>
          <!-- <a href="https://github.com/yenchiah/project-website-template">GitHub</a> -->
        </div>
      </div>
    </div>
  </div>
  <div class="content-container">
    <h1 style="text-align:center">Deep Low Light Image Enhancement</h1>
    <div class="content">
      <div class="content-table flex-column">
        <!-------------------------------------------------------------------------------------------->
        <!--Start Intro-->
        <div class="flex-row" style="align-items: center; justify-content: center;">
          <div class="flex-item flex-column">
            <figure>
              <img class="image" src="img/head_image.png">
              <figcaption style="text-align: center;">Source: <a href="https://github.com/VITA-Group/EnlightenGAN/blob/master/assets/show_3.png">GitHub</a></figcaption>
            </figure>
          </div>
        </div>
        <div class="flex-row" style="align-items: center; justify-content: center;">
            <p class="text text-large" style="text-align: center;">
              <a target="_blank" href="https://computing.ece.vt.edu/~pkmandke">Prathamesh Mandke</a>, pkmandke AT vt DOT edu<br>
              <a target="_blank" href="mailto:japss96@vt.edu">Japnit Singh Sethi</a>, japss96 AT vt DOT edu<br>
              <!-- Fall 2020 ECE-5554 Computer Vision: Course Project<br> -->
              <a target="_blank" href="https://ece.vt.edu">Department of Electrical and Computer Engineering</a><br>
              <a target="_blank" href="https://vt.edu">Virginia Tech</a><br>
            </p>
        </div>
        <!--End Intro-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Text Only-->
        <div id="abstract" class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">Abstract</h2>
            <hr>
            <p class="text">
              In this work, we consider the low light image enhancement problem of recovering an enhanced normal light version of a low contrast image which suffers from poor visibilty.
              Image enhancement is an inherently ill-posed problem since a given low light image can have many possible normal light equivalents.
              Further, the presence of noise along with spatial variations in contrast and brightness pose additional challenges in solving this problem.
              <!-- Low light image enhancement finds widespread applications in domains such as autonomous systems and surveillance where mission critical computer vision systems rely on images captured in low light regions for decision making. -->
            </p>
            <p class="text">
              We seek to explore, compare and contrast various deep learning based approaches to the problem in supervised and unsupervised settings.
              In our experiments, we obtain performance benchmarks with 3 methods viz., 1) <a href="https://pubmed.ncbi.nlm.nih.gov/9848052/">CLAHE</a>, which is a conventional (non deep learning) histogram equalization approach 2) <a href="https://arxiv.org/pdf/1906.06972.pdf">EnlightenGAN</a> an unsupervised state-of-the-art low light enhancement model based on GANs and 3) multiple variations of U-Net based auto-encoder architectures with certain modifications.
              We observe that the Generative model performs best across these methods while CLAHE performs worst.
              The auto-encoder based architectures are able to retrieve most information from the low light images but the images look less natural.
              We provide qualitative as well as quantitave benchmarks to support these claims.
              <!-- In particular, various flavors of deep convolutional auto-encoder based architectures will be compared to adversarial models -->
              <!-- These approaches (as elaborated in the next section) will be evaluated qualitatively as well as quantitavely using metrics such as PSNR, SSIM and RMSE.  -->
            </p>
          </div>
        </div>
        <!--End Text Only-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Introduction-->
        <div id="introduction" class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">Introduction</h2>
            <hr>
            <p class="text">
              Low light image enhancement is a widely studied problem in Computer Vision, where the goal is to recover an enhanced normal light version of an image with low contrast or visibility.
              Low light image enhancement finds widespread applications in domains such as autonomous driving and surveillance where mission critical computer vision systems rely on images captured in low lighting conditions for decision making.
              As mentioned earlier, this task is inherently ill-posed since a given low-light image can have multiple potential normal light equivalents.
              A simple example could be where a snapshot of a mountain range taken at night (pitch dark) could map to a normal light version during peak sunshine or at dusk.
              Besides, other artifacts such as image noise or spatially varying brightness and contrast pose additional difficulties.
              For example, certain regions of an image may need to be enhanced more than others in terms of contrast.
              Thus, a conventional Computer Vision approach that increases the overall image contrast may not give visually pleasing results.
            </p>
          </div>
        </div>
        <!--End Introduction Only-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Approach-->
        <div id="approach" class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">Approach</h2>
            <hr>
            <p class="text">
              Most approaches to the image enhancement problem can be categorized as either histogram based methods (such as <a href="https://pubmed.ncbi.nlm.nih.gov/9848052/">CLAHE</a>) or learning based methods.
              Learning based approaches can further be subdivided into supervised or unsupervised based on whether paired low-light and normal-light images are available.
              Further, deep learning methods are based on either the convolutional auto-encoder framework or the Generative Adversarial Models (GANs) based framework.
            </p>
            <!-- <figure>
              <img class="image" src="img/unet_base.png">
              <figcaption style="width: 100%;text-align: center;">The baseline U-net architecture. Source: <a href="https://arxiv.org/pdf/1505.04597.pdf">Arxiv</a></figcaption>
            </figure> -->
            <p class="text">
              As mentioned earlier, the goal of this work is to compare and contrast various different methods across each of the aforementioned approaches to the problem.
              To this end, we choose to work with: 1) <a href="https://pubmed.ncbi.nlm.nih.gov/9848052/">CLAHE</a> - an adaptive histogram equalization based technique, 2) <a href="https://arxiv.org/pdf/1906.06972.pdf">EnlightenGAN</a> - an unsupervised GAN based framework and 3) multiple variations of the U-Net based auto-encoder framework which has shown great success for image-to-image translation problems.
              We present the results of benchmarking these methods supported with qualitative and quantitave comparisons.
              For CLAHE, we use the <a href="https://docs.opencv.org/master/d5/daf/tutorial_py_histogram_equalization.html">OpenCV implementation</a> of the algorithm.
              To experiment with EnlightenGAN, we use the codebase provided by the authors (<a href="https://github.com/VITA-Group/EnlightenGAN">GitHub</a>) and retrain the model using 3 NVIDIA GPUs based on the technique described in the <a href="https://arxiv.org/pdf/1906.06972.pdf">paper</a>.
              We implement the U-Net based auto-encoder variants from scratch using the PyTorch framework in Python.
              A brief overview of the aforementioned approaches follows.
            </p>
            <h3>CLAHE and EnlightenGAN</h3>
            <figure>
              <img class="image" style="display: block;margin-left: auto; margin-right: auto;width: 60%;" src="img/clahe_teaser.png">
              <figcaption style="width: 100%;text-align: center;">The CLAHE technique - histogram equalization and contrast limiting. Source: <a href="https://en.wikipedia.org/wiki/Adaptive_histogram_equalization">Wikipedia</a></figcaption>
            </figure>
            <p class="text">
              CLAHE (Contrast Limited Adaptive Histogram Equalization) improves over vanilla histogram equalization by considering small patches of the image to perform local equalization instead of globally over the entire image.
              This helps avoid any unwanted excess enhancement in regions with very high or very low contrast.
              Further, contrast limiting (contrast values beyond a threshold are clipped) is applied to reduce the effect of noise amplification as shown in the figure above.
            </p>
            <figure>
              <img class="image" style="display: block;margin-left: auto; margin-right: auto;width: 100%; height: 320px;" src="img/engan_teaser.png">
              <figcaption style="width: 100%;text-align: center;">EnlightenGAN framework. Source: <a href="https://arxiv.org/pdf/1906.06972.pdf">ArXiv</a></figcaption>
            </figure>
            <p class="text">
              EnlightenGAN is an unsupervised GAN based low light image enhancment method proposed by <a href="https://arxiv.org/pdf/1906.06972.pdf">Jiang et al.</a>.
              The framework learns to enhance low light images without any paired supervision at training time. 
              The main highlights of this work are a self-attention module in the Generator model along with a dual global-local discriminator structure that helps the model handle both the fine and coarse details in the image with relative ease.
            </p>
            <h3>U-Net based auto-encoders</h3>
            <figure>
              <img class="image" style="display: block;margin-left: auto; margin-right: auto;width: 100%; height: 420px;" src="img/tconv_arch_big.png">
              <figcaption style="width: 100%;text-align: center;">The U-Net like auto-encoder with Transpose Convolutions for Upsampling</figcaption>
            </figure>
            <p class="text">
              To explore the problem in a supervised setting, we implement <a href="https://arxiv.org/abs/1505.04597">U-Net</a> based auto-encoder architectures and explore multiple versions of the same.
              Image enhancement falls under a wider class of problems known as image-to-image translation in which the U-Net architecture has shown great success.
              In this work, we explore two variations of the original U-net architecture.
              While the original model uses feature concatenation in the residual connections, we instead perform arithmatic addition of the corresponding feature maps in the encoder and decoder drawing inspiration from the <a href="https://arxiv.org/abs/1512.03385">ResNet</a> architecture.
              This helps reduce the feature maps sizes in the decoder while also helping the decoding process with features extracted from the original image.
              In addition, we explore the effect of two upsampling strategies in the decoder namely transpose convolutions and bilinear upsampling.
              While bilinear upsampling has been a widely used upsampling strategy, the transpose convolution operation (popularized by <a href="https://arxiv.org/pdf/1511.06434.pdf">the DCGAN paper</a>) serves as a learnable upsampling layer in convolutional auto-encoders.
              As shown in the figure above, the overall architecture mainly consists of two parts, the encoder which consists of convolutional layers which use a stride of 2 to achieve the downsampling of the feature maps and the decoder which upsamples the feature map from the encoder using either the transpose convolutional layers (as shown) or bilinear upsampling.
              The skip connections, which add together feature maps from corresponding levels of the encoder and the decoder help the decoder in retrieving features of the input image which make reconstruction easier.
              In addition, the final output image of the decoder is added (element-wise) with the original input image to generate the normal light output.
              This way, the model only learns to predict the difference between the input low-light and the output normal-light image which is often an easier task than predicting the output image directly.
            </p>
            <figure>
              <img class="image" style="display: block;margin-left: auto; margin-right: auto;width: 100%; height: 420px;" src="img/bilinear_arch_big.png">
              <figcaption style="width: 100%;text-align: center;">The U-Net like auto-encoder with Bilinear Upsampling for Upsampling</figcaption>
            </figure>
            <!-- <p class="text">
              Finally, the auto-encoder based architectures will be compared against a GAN based unpaired image enhancement model called <a href="https://arxiv.org/pdf/1906.06972.pdf">EnlightenGAN</a> and <a href="https://pubmed.ncbi.nlm.nih.gov/9848052/">CLAHE</a> - a histogram based method .
              In many practical scenarios, it often difficult to obtain sufficient pairs of low light images with their normal light counter parts.
              EnlightenGAN is an adversarial technique that learns a model for image enhancement in such an unsupervised setting.
              CLAHE is an adaptive histogram equalization method that improves image contrast while reducing undesired noise amplification.
              Thus, we seek to explore paired and unpaired settings for image enhancement along with learning and histogram based methods.
            </p> -->
          </div>
        </div>
        <!--End Text Only-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Experiments and Results-->
        <div id="exp-and-results" class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">Experiments and Results</h2>
            <hr>
            <p class="text">
              In this section, we present the experimental setup and results of our experiments with the CLAHE technique, EnlightenGAN and the U-Net based auto-encoders.
            </p>
            <h3>Experimental Setup</h3>
            <h4>CLAHE and ENGAN</h4>
            <p class="text">
              To begin with, we work with Contrast Limited Adaptive Histogram Equalization which is a conventional computer vision algorithm as described in the previous section.
              This technique does not require any data for training.
              We use the <a href="https://docs.opencv.org/master/d5/daf/tutorial_py_histogram_equalization.html">OpenCV's python based</a> implementation of CLAHE.
              There are two parameters viz., clip limit for the contrast threshold and the grid or window size to use for applying histogram equalization.
            </p>
            <p class="text">
              To experiment with EnlightenGAN, we reuse the author's <a href="https://github.com/VITA-Group/EnlightenGAN">codebase</a> which is based on the <a href="https://pytorch.org">PyTorch</a> deep learning framework.
              (Note that we do not attach the code or the dataset for ENGAN with the submission, since we have not made any modifications from the author's original code.)
              The author's have curated a custom unpaired dataset with 914 low light and 1016 normal light images which we use for training.
              We re-train the EnlightenGAN model with the default setting for the EnlightenGAN architecture as described <a href="https://github.com/VITA-Group/EnlightenGAN/blob/master/scripts/script.py">here</a>.
              The training takes ~3hrs when distributed accross 3 NVIDIA TITAN RTX GPUs. The Figure below shows the training curve for 200 epochs visualized using the <i><a href="https://github.com/facebookresearch/visdom">visdom</a></i>&nbsp;&nbsp;tool.
            </p>
            <figure>
              <img class="image" style="display: block;margin-left: auto; margin-right: auto;width: 70%; height: 320px;" src="img/engan_loss.png">
              <figcaption style="width: 100%;text-align: center;">EnlightenGAN training Loss Curves.G_A: Generator Loss, D_A: Global Discrimintor Loss, D_P: Patch Discriminator Loss, vgg: VGG Loss. Note that GAN loss curves do not necessarily reduce. The training is often complete when both G and D loss remain constant as niether of the 2 networks can learn from each other.</figcaption>
            </figure>
            <h3>U-Net based auto-encoders</h3>
            <h4>Programmer's Guide</h4>
            <p class="text">
              We implement the different variations of the U-net based auto-encoder architectures (described in the Approach section above) from scratch in Python (v3.7) using the <a href="https://pytorch.org/">PyTorch</a> (v1.6) deep learning library.
              We adopt the codebase structure from <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">this</a> GitHub repository which makes it easy to absract away common contructs of dataset loading as well as model training, evaluation and visualization.
              In particular, we adopt the base classes for different entities (such as models and dataloaders) and the options handler for all our experiments with the U-Net auto-encoders.
              Based on this structure, we implement a model class for the auto-encoder (models/autoencoder_model.py) which includes code for initializing the model along with training and testing it based on custom options.
              We also implement a generic dataloader for any dataset having training/validation/testing data (see data/trainval_dataloader.py).
              The LoL dataset can be loaded by our custom dataloader defined in data/lol_dataset.py.
              We also implement the two versions of the auto-encoder described earlier (see models/networks.py).
              train.py and test.py are generic scripts that can be used to run an instance of training/testing the model. 
              We train all models on a compute node with NVIDIA Titan RTX GPUs.
            </p>
            <h4>Implementation Details</h4>
            <div class="flex-row-space-between">
              <div class="flex-item flex-column">
                <img class="image max-width-400" style="height: 260px; width: 340px;" src="img/ex3tr5_rmse.png">
                <p class="image-caption">Training/Validation RMSE Loss</p>
              </div>
              <div class="flex-item flex-column">
                <img class="image max-width-400" style="height: 260px; width: 340px;" src="img/ex3tr5_psnr.png">
                <p class="image-caption">Training/Validation PSNR</p>
              </div>
              </div>
            <p class="text">
              There are two variants of the model with bilinear upsampling and transpose convolutions as the upsampling strategies.
              We train each of these models with two loss functions: 1) Only the Root Mean Square Error (RMSE) between the predicted image and the ground truth normal light image and 2) RMSE along with the Structure Similarity Index.
              The models are trained to minimize the Root Mean Square Error between the output and the ground truth normal light image while (optionally) maximizing the Structure Similarity Index Measure (SSIM).
              (Note: The trainable version of the SSIM <a href="https://github.com/Po-Hsun-Su/pytorch-ssim">at this URL</a> is used in our work.)
              Thus, the model is trained to minimize: Loss = RMSE + lambda * (1 - SSIM) which is the overall loss function where the weight of the SSIM term can be controlled with the lambda parameter.
              Note that while the RMSE forces pixel wise similarity between the predicted and ground truth natural light images, the SSIM strives to achieve more better visual pleasing image enhancement.
              This results in 4 different training configurations: 1) Bilinear upsampling + RMSE Loss 2) Bilinear upsampling + RMSE and SSIM Loss 3) Transpose Convolutional + RMSE Loss and 4) Transpose Convolution + RMSE and SSIM Loss.
              Instances all these configurations are trained using the 485 images from the training set of the LOw-Light (LOL) Dataset curated by <a href="https://arxiv.org/abs/1808.04560">Wei et al.</a> that is available publicly at <a href="https://drive.google.com/open?id=157bjO1_cFuSd0HWDUuAmcHRJDVyWpOxB">this HTTPS URL</a>.
              The data is split into 450/485 images for training and 35 for validation with image resized to a fixed size of 320x320px while maintaining the aspect ratio.
              This resizing is achieved by the function *MakeSquared* implemented in data/utils.py.
              The RMSE Loss along with the Peak Signal to Noise Ratio (PSNR) for training/validation for the Bilinear+RMSE configuration is shown in the figure above.
              (We do not present the loss curves for all 4 configurations to save space and avoid redundancy.)
            </p>
            <h3>Metrics</h3>
            <p class="text">
              Along with qualitative visualization, we rely on 2 commonly used quantitave metrics viz. Root Mean Square Error (RMSE) and Structure Similarity Index (SSIM) for evaluating our results.
              The RMSE is a pixel wise mean square error between the true and predicted normal light image.
              A lower value of RMSE indicates better similarity between two images.
              The Structure Similarity Index (<a href="https://www.cns.nyu.edu/~lcv/ssim/">SSIM</a>) measures the perceptual similarity between two images and gives a a more general idea of how similar two images appear to a viewer.
              The value of SSIM ranges between 0 and 1 with a higher value indicating better structural similarity.
              We use <a href="https://scikit-learn.org/stable/">Sci-kit Learn's</a> implementation of SSIM and RMSE in our work. 
            </p>
            <div class="flex-row-space-between">
              <div class="flex-item flex-column">
                <img class="image max-width-400" style="width: 320px;" src="img/clip_limit_search_rmse.png">
                <p class="image-caption">Mean RMSE as a function of clip limit (contrast threshold)</p>
              </div>
              <div class="flex-item flex-column">
                <img class="image max-width-400" style="width: 320px;" src="img/clip_limit_search_ssim.png">
                <p class="image-caption">Mean SSIM as a function of clip limit (contrast threshold)</p>
              </div>
            </div>
            <p class="text">
              In order to choose optimal contrast limit value for CLAHE, we perform a hyper-parameter search over the 485 train images from the LoL paired image dataset curated by <a href="https://arxiv.org/abs/1808.04560">Wei et al.</a>.
              We fix the grid or window size for CLAHE to 2 and plot the mean RMSE and SSIM across all images as a function of the clip limit to pick the optimal value of clip limit such that RMSE is minimum while SSIM is maximum.
              The plots are shown in the Figure above.
              The code to generate these plots along with the CLAHE algorithm has been submitted. 
            </p>
            <h3>Results</h3>
            <p class="text">
              In order to evaluate these different techniques, we use the test subset of the LOw-Light (LOL) Dataset curated by <a href="https://arxiv.org/abs/1808.04560">Wei et al.</a> that is available publicly at <a href="https://drive.google.com/open?id=157bjO1_cFuSd0HWDUuAmcHRJDVyWpOxB">this HTTPS URL</a>.
              This dataset consists of paired low light and normal light images which we use to compute the metrics described below.
            </p>
            <div class="gallery">
              <a href="javascript:void(0)" class="flex-column"><img src="img/low/1.png">
                <div>Low Light</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/clahe/1.png">
                <div>CLAHE</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/engan/1_fake_B.png">
                <div>ENGAN</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/high/1.png">
                <div>Ground Truth</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/low/146.png">
                <div>Low Light</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/clahe/146.png">
                <div>CLAHE</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/engan/146_fake_B.png">
                <div>ENGAN</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/high/146.png">
                <div>Ground Truth</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/low/179.png">
                <div>Low Light</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/clahe/179.png">
                <div>CLAHE</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/engan/179_fake_B.png">
                <div>ENGAN</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/high/179.png">
                <div>Ground Truth</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/low/748.png">
                <div>Low Light</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/clahe/748.png">
                <div>CLAHE</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/engan/748_fake_B.png">
                <div>ENGAN</div>
              </a>
              <a href="javascript:void(0)" class="flex-column"><img src="img/high/748.png">
                <div>Ground Truth</div>
              </a>
            </div>
            <p class="text">
              The gallery of images above displays the qualitative results of the 2 methods in consideration.
              It can be seen that ENGAN despite being an unsupervised technique, performs significantly better than CLAHE which is based on histogram equalization.
              To obtain these results, we select a window size of 2 and select the clip limit (contrast threshold) of 30. based on the hyper-parameter search described in the above sub-section.
              In particular, it can be seen that with the window size fixed, the RMSE has a minimum at a clip limit of 30. while the SSIM steadily falls with the clip limit.
              Thus, we select 30 as an optimal value of the contrast threshold and apply CLAHE on all the images from the evaluation subset of the LoL dataset.
              Some of the images are shown in the image gallery above.
              We train ENGAN on the original dataset curated by the author's consisting of 914 low light and 1016 normal light unpaired images and apply the trained model on the same evaluation subset of the LoL dataset.
              The 3rd column in the image gallery shows the results of ENGAN.
            </p>
            <p class="text">
              To obtain quantitavely results, we compute the mean SSIM and RMSE over the evaluation dataset for both the methods - CLAHE and ENGAN.
              <table style="border: 1px solid black;margin-left:auto; margin-right:auto; ">
                <tr style="border: 1px solid black;">
                  <td style="border: 1px solid black;"></td>
                  <td style="border: 1px solid black;">RMSE</td>
                  <td style="border: 1px solid black;">SSIM</td>
                </tr>
                <tr style="border: 1px solid black;">
                  <td style="border: 1px solid black;">CLAHE</td>
                  <td style="border: 1px solid black;">49.81</td>
                  <td style="border: 1px solid black;">0.4455</td>
                </tr>
                <tr style="border: 1px solid black;">
                  <td style="border: 1px solid black;">ENGAN</td>
                  <td style="border: 1px solid black;">35.69</td>
                  <td style="border: 1px solid black;">0.671</td>
                </tr>
              </table>
              It is interesting to observe that despite the qualitative difference between the two methods being significant, quantitavely they do not seem to differ as much.
              For instance, the Mean SSIM value over the evaluation dataset (LoL) is 0.4455 for CLAHE while being 0.671 for ENGAN.
              Similarly, the difference in RMSE is not significant either.
              Thus, while qualitatively ENGAN performs significantly better than CLAHE the quantitave numbers suggest that the techniques do not differ much and there is scope for better results.
              We seek to investigate auto-encoder based methods moving forward and attempt to compare those with ENGAN and CLAHE.
            </p>
            <!-- <h4>Experimental Setup</h4>
            <p class="text">
              In this work, we will implement the U-Net based convolutional auto-encoder architecture in Python using the <a href="https://pytorch.org">PyTorch</a> framework along with the code for training and testing pipelines. 
              The <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">pytorch-CycleGAN-and-pix2pix</a> codebase structure and base classes will be extended to manage experiments and checkpoints.
              We will use the <i><a href="https://github.com/facebookresearch/visdom">visdom</a></i>&nbsp;&nbsp;tool for visualization and the <i><a href="https://github.com/marcsantiago/dominate">dominate</a></i>&nbsp;&nbsp;API for managing HTML pages dynamically as needed.
            </p>
            <h4>Datasets</h4>
            <p class="text">
              We will train and evaluate our models on the LOw-Light (LOL) Dataset curated by <a href="https://arxiv.org/abs/1808.04560">Wei et al.</a> that is available publicly at <a href="https://drive.google.com/open?id=157bjO1_cFuSd0HWDUuAmcHRJDVyWpOxB">this HTTPS URL</a>.
              The dataset consists of 500 pairs of low/normal light images across a variety of scenes such as houses, campuses, streets, etc. providing a diverse range of images.
              EnlightenGAN will be trained on the custom unpaired dataset curated by the authors (<a href="https://arxiv.org/pdf/1906.06972.pdf">Jiang et al.</a>) that can be accessed at <a href="https://drive.google.com/drive/folders/1fwqz8-RnTfxgIIkebFG2Ej3jQFsYECh0?usp=sharing">this HTTPS URL</a>.
              This dataset contains manually curated and examined unpaired set of 914 low light and 1016 normal light images collected by the authors from several sources. 
            </p>
            <h4>Experiments</h4>
            <p class="text">
              In this work, we will experiment with the U-Net based convolutional auto-encoder architecture with the following modifications:
              1) Replace concatenation in the residual layers with skip connections (inspired by <a href="https://arxiv.org/abs/1512.03385">He et al.</a>)
              2) Explore 2-strided transpose convolutions as well as bilinear interpolation in the decoder
              3) Explore RMSE and SSIM based loss functions
              4) Train the EnlightenGAN model for unpaired image enhancement
              5) Compare and contrast results with the CLAHE algorithm<br>
              We will use off-the-shelf implementations for training the <a href="https://github.com/VITA-Group/EnlightenGAN">EnlightenGAN</a> model as well as the <a href="https://docs.opencv.org/master/d5/daf/tutorial_py_histogram_equalization.html">OpenCV implementation</a> of the CLAHE algorithm.
            </p>
            <h4>Metrics</h4>
            <p class="text">
              The results of various experiments will be compared using commonly used metrics for image-to-image translation problems.
              The pixel-wise mean square error will be used to train the auto-encoder model via the back-propagation algorithm.
              The Structure Similarity Index (<a href="https://www.cns.nyu.edu/~lcv/ssim/">SSIM</a>) measures the perceptually similarity between two images and will be used to study how well the model can recover the original image feature details.
              Since image enhancement is a highly ill-posed problem with multiple possible ground truth enhanced versions, we will also strive to perform a robust qualitative comparison of the images.
              Thus, we seek to compare and contrast the aforementioned methods for image enhancement in the low light setting.
            </p> -->
          </div>
        </div>
        <!--End Text Only-->
        <!-------------------------------------------------------------------------------------------->
        <div id="conclusion" class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">Conclusion and Future Work</h2>
            <hr>
            <p class="text">
              This report has described a qualitative as well as quantitave comparison between two approaches towards the Low light Image Enhancement problem.
              On the one hand, we explore CLAHE - a conventional computer vision algorithm based on histogram equalization, while ENGAN is a state-of-the-art unsupervised image enhancement technique.
              Qualitative results indicate that ENGAN, when trained with ~1k images, performs significantly better than CLAHE, with the images closely resembling the ground truth normal light versions.
              The results from CLAHE do not appear to recover the color aspects of the original image as well as ENGAN. 
              Further, quantitave comparisons between RMSE and SSIM indicate that CLAHE performs worse than ENGAN albeit not by a significant margin.
              In our work moving forward, we seek to focus on multiple variants of the U-Net based convolutional auto-encoder architecture and compare and contrast their performance with CLAHE and ENGAN.
            </p>
          </div>
        </div>
        <!--End Text Only-->
        <!-------------------------------------------------------------------------------------------->
        <div id="references" class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">References</h2>
            <hr>
            <p class="text">
              [1] Ronneberger O., Fischer P., Brox T. (2015) U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab N., Hornegger J., Wells W., Frangi A. (eds) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, vol 9351. Springer, Cham. https://doi.org/10.1007/978-3-319-24574-4_28
            </p>
            <p class="text">
              [2] Jiang, Yifan, et al. "Enlightengan: Deep light enhancement without paired supervision." arXiv preprint arXiv:1906.06972 (2019).
            </p>
            <p class="text">
              [3] He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.
            </p>
            <p class="text">
              [4] Pisano ED, Zong S, Hemminger BM, DeLuca M, Johnston RE, Muller K, Braeuning MP, Pizer SM. Contrast limited adaptive histogram equalization image processing to improve the detection of simulated spiculations in dense mammograms. J Digit Imaging. 1998 Nov;11(4):193-200. doi: 10.1007/BF03178082. PMID: 9848052; PMCID: PMC3453156.
            </p>
            <p class="text">
              [5] Wei, Chen, et al. "Deep retinex decomposition for low-light enhancement." arXiv preprint arXiv:1808.04560 (2018).
            </p>
            <p class="text">
              [6] Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, "Image quality assessment: From error visibility to structural similarity," IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr. 2004.
            </p>
            <p class="text">
              [7] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2 (NIPS'14). MIT Press, Cambridge, MA, USA, 2672–2680.
            </p>
            <p class="text">
              [8] URL: <a href="https://docs.opencv.org/master/d5/daf/tutorial_py_histogram_equalization.html">https://docs.opencv.org/master/d5/daf/tutorial_py_histogram_equalization.html</a>
            </p>
            <p class="text">
              [9] Visdom tool by Facebook Research. URL: <a href="https://github.com/facebookresearch/visdom">https://github.com/facebookresearch/visdom</a>
            </p>
            <p class="text">
              [10] PyTorch library. URL: <a href="https://pytorch.org">https://pytorch.org</a>
            </p>
            <p class="text">
              [11] Radford, A., Metz, L. & Chintala, S. (2015). Unsupervised Representation Learning with Deep Convolutional Generative
              Adversarial Networks. <a href="https://arxiv.org/pdf/1511.06434.pdf">URL</a>
            </p>
          </div>
        </div>
        <!--End Text Only-->
        <!-------------------------------------------------------------------------------------------->
      </div>
    </div>
  </div>

  <div style="text-align: center">Website Source: <a href="https://github.com/yenchiah/project-website-template">GitHub</a></div>
</body>

</html>